{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "895426bc-5f26-4fbb-a3bf-e666aa7e810c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdbf0ed871d54e6f82d528968370c243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lbwdruid/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab/config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011b75bed21f4e4f8d2c716f8ef7dd88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lbwdruid/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab/generation_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09f1fe2e2294f26b782a801b38670d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lbwdruid/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56344b0a38bd4fc7b93819ffe969271a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lbwdruid/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab/tokenizer.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575339a102744bac87526c1a0c0de09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lbwdruid/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "HUGGING_FACE_API_KEY = os.environ.get(\"HUGGING_FACE_API_KEY\")\n",
    "print(os.environ.get(\"HUGGING_FACE_API_KEY\"))\n",
    "\n",
    "model_id = \"google/flan-t5-small\"\n",
    "filenames = [\n",
    "        \"config.json\", \"generation_config.json\", \"special_tokens_map.json\", \"tokenizer.json\", \"tokenizer_config.json\",\n",
    "]\n",
    "\n",
    "for filename in filenames:\n",
    "        downloaded_model_path = hf_hub_download(\n",
    "                    repo_id=model_id,\n",
    "                    filename=filename,\n",
    "                    token=HUGGING_FACE_API_KEY\n",
    "        )\n",
    "        print(downloaded_model_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf1606fb-bca5-48fb-8081-a1ead2e72594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "local_model_dir = \"/home/lbwdruid/.cache/huggingface/hub/models--google--flan-t5-small/snapshots/0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_dir, legacy=False)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "# Initialize the pipeline for text generation\n",
    "llm_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a6e9ff5-1917-4ae6-8f33-8fb779c0f407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted labels: T5,T7-T9\n"
     ]
    }
   ],
   "source": [
    "# Define the radiology report and bone labels\n",
    "bone_list = [\"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\", \"C7\",\n",
    "             \"T1\", \"T2\", \"T3\", \"T4\", \"T5\", \"T6\", \"T7\", \"T8\", \"T9\", \"T10\", \"T11\", \"T12\",\n",
    "             \"L1\", \"L2\", \"L3\", \"L4\", \"L5\", \"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]\n",
    "report = \"Plan to treat C2-T5, and T7-MidT9 disease.\"\n",
    "prompt = \"\"\"\n",
    "I have a radiologist report, which includes where we should treat the patient on the Spine.\n",
    "Anatomically, the bones in the spine are labeled by these labels (these labels are in order, where C1 represents the top bone, and S5 represents the bottom bone):\n",
    "{C1, C2, C3, C4, C5, C6, C7, T1, T2, T3, T4, T5, T6, T7, T8, T9, T10, T11, T12, L1, L2, L3, L4, L5, S1, S2, S3, S4, S5}.\n",
    "Could you please help me to extract bone labels from the report below within the quotation marks, and keep the answer to only the labels that I provided?\n",
    "It may include a range of bones, please refer to the list that I provided, and find all bones.\n",
    "No explanation is needed, just the labels separated by commas please.\n",
    "\n",
    "\"Plan to treat C2-T5, and T7-MidT9 diease.\"\n",
    "\"\"\"\n",
    "\n",
    "prompt = '''Human's bones in spine are labeled by these labels \n",
    "(these labels are in order, where C1 represents the top bone, and S5 represents the bottom bone): \n",
    "{C1, C2, C3, C4, C5, C6, C7, T1, T2, T3, T4, T5, T6, T7, T8, T9, T10, T11, T12, L1, L2, L3, L4, L5, S1, S2, S3, S4, S5}.\n",
    "The '-' in the following sentence represents a range of bones, the letters before it represent the top bone of the range,\n",
    "and the letters after it represent the bottom bone of the range.\n",
    "Remove '-' and print every bone in the range one by one explicitly.\n",
    "\n",
    "\"C2-T5,T7-T9\"\n",
    "'''\n",
    "\n",
    "# Generate response using LLaMA 3\n",
    "response = llm_pipeline(prompt, max_length=500, temperature=0.7)\n",
    "\n",
    "# Process the response to filter the correct bone labels\n",
    "generated_text = response[0]['generated_text']\n",
    "print(\"Extracted labels:\", generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
