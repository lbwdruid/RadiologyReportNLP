{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73d0e682-7be9-4c3d-adbc-ce8454350e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n",
      "/home/lbwdruid/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "/home/lbwdruid/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/generation_config.json\n",
      "/home/lbwdruid/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json\n",
      "/home/lbwdruid/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json\n",
      "/home/lbwdruid/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "HUGGING_FACE_API_KEY = os.environ.get(\"HUGGING_FACE_API_KEY\")\n",
    "print(os.environ.get(\"HUGGING_FACE_API_KEY\"))\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "filenames = [\n",
    "        \"config.json\", \"generation_config.json\", \"special_tokens_map.json\", \"tokenizer.json\", \"tokenizer_config.json\",\n",
    "]\n",
    "\n",
    "for filename in filenames:\n",
    "        downloaded_model_path = hf_hub_download(\n",
    "                    repo_id=model_id,\n",
    "                    filename=filename,\n",
    "                    token=HUGGING_FACE_API_KEY\n",
    "        )\n",
    "        print(downloaded_model_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "924a7fcd-548c-429b-9c48-d6ffaa8fbe8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd60b9be974946e8825446a69fc976f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "local_model_dir = \"/home/lbwdruid/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_dir, legacy=False)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Initialize the pipeline for text generation\n",
    "llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344b6366-54ba-4ff7-9f17-e28e6eb8576b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Define the radiology report and bone labels\n",
    "bone_list = [\"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\", \"C7\",\n",
    "             \"T1\", \"T2\", \"T3\", \"T4\", \"T5\", \"T6\", \"T7\", \"T8\", \"T9\", \"T10\", \"T11\", \"T12\",\n",
    "             \"L1\", \"L2\", \"L3\", \"L4\", \"L5\", \"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]\n",
    "report = \"Plan to treat C2-T5, and T7-MidT9 disease.\"\n",
    "prompt = f\"\"\"\n",
    "I have a radiology report: \"{report}\"\n",
    "Please extract the spine bone labels mentioned in this report, using only labels from the list:\n",
    "{', '.join(bone_list)}. Noted these bones are already sorted in order. Provide the labels as a comma-separated list without explanations or additional text.\n",
    "\"\"\"\n",
    "\n",
    "# Generate response using LLaMA 3\n",
    "response = llm_pipeline(prompt, max_length=500, temperature=0.7)\n",
    "\n",
    "# Process the response to filter the correct bone labels\n",
    "generated_text = response[0]['generated_text'][len(prompt):]\n",
    "print(\"Extracted labels:\", generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ff2dc-8520-49fb-9787-61d0cf6b7f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Post-process to extract valid bone labels\n",
    "matches = re.findall(r'\\b(C\\d|T\\d{1,2}|L\\d|S\\d)\\b(?:-(C\\d|T\\d{1,2}|L\\d|S\\d))?', generated_text)\n",
    "extracted_bones = set()\n",
    "\n",
    "# Expand ranges like C2-T5 and collect individual bones\n",
    "for match in matches:\n",
    "    start_bone = match[0]\n",
    "    end_bone = match[1] if match[1] else start_bone\n",
    "    start_idx = bone_list.index(start_bone)\n",
    "    end_idx = bone_list.index(end_bone)\n",
    "    extracted_bones.update(bone_list[start_idx:end_idx + 1])\n",
    "\n",
    "# Output the extracted bones in a sorted manner\n",
    "extracted_bones = sorted(extracted_bones, key=lambda x: bone_list.index(x))\n",
    "print(\"Final extracted labels:\", ', '.join(extracted_bones))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
